{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ensemble of YOLOv5 s6 and n6 model\nTo improve the performace of COTS prediction we create an ensemble of the YOLOv5 s6 and n6 models using the weighted box fusion technique","metadata":{}},{"cell_type":"markdown","source":"## Install Libraries","metadata":{"papermill":{"duration":0.033246,"end_time":"2022-04-22T02:40:41.128819","exception":false,"start_time":"2022-04-22T02:40:41.095573","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# !pip download bbox-utility\n# !pip download ensemble-boxes","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":0.041853,"end_time":"2022-04-22T02:40:41.20223","exception":false,"start_time":"2022-04-22T02:40:41.160377","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:10:45.778001Z","iopub.execute_input":"2022-04-25T16:10:45.778740Z","iopub.status.idle":"2022-04-25T16:10:45.797958Z","shell.execute_reply.started":"2022-04-25T16:10:45.778631Z","shell.execute_reply":"2022-04-25T16:10:45.797238Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install --no-index --find-links '/kaggle/input/' '/kaggle/input/icevision-essentials/icevision-repos/repos/loguru-0.6.0-py3-none-any.whl'\n!pip install --no-index --find-links '/kaggle/input/modules/' '/kaggle/input/modules/bbox_utility-1.0.13-py3-none-any.whl'\n!pip install --no-index --find-links '/kaggle/input/modules/' '/kaggle/input/modules/ensemble_boxes-1.0.9-py3-none-any.whl'","metadata":{"papermill":{"duration":32.573531,"end_time":"2022-04-22T02:41:13.807061","exception":false,"start_time":"2022-04-22T02:40:41.23353","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:10:45.832683Z","iopub.execute_input":"2022-04-25T16:10:45.833137Z","iopub.status.idle":"2022-04-25T16:11:13.507961Z","shell.execute_reply.started":"2022-04-25T16:10:45.833100Z","shell.execute_reply":"2022-04-25T16:11:13.507077Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Import Libraries","metadata":{"papermill":{"duration":0.037393,"end_time":"2022-04-22T02:41:13.880401","exception":false,"start_time":"2022-04-22T02:41:13.843008","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\n\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\nsys.path.append('/kaggle/input/weightedboxesfusion/')\n\nfrom IPython.display import display\nfrom ensemble_boxes import *\nfrom PIL import Image\nimport torch","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.529984,"end_time":"2022-04-22T02:41:14.447647","exception":false,"start_time":"2022-04-22T02:41:13.917663","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:11:13.510984Z","iopub.execute_input":"2022-04-25T16:11:13.511356Z","iopub.status.idle":"2022-04-25T16:11:15.804526Z","shell.execute_reply.started":"2022-04-25T16:11:13.511314Z","shell.execute_reply":"2022-04-25T16:11:15.803531Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'","metadata":{"papermill":{"duration":0.044344,"end_time":"2022-04-22T02:41:15.683462","exception":false,"start_time":"2022-04-22T02:41:15.639118","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:11:15.805685Z","iopub.execute_input":"2022-04-25T16:11:15.805908Z","iopub.status.idle":"2022-04-25T16:11:15.809971Z","shell.execute_reply.started":"2022-04-25T16:11:15.805881Z","shell.execute_reply":"2022-04-25T16:11:15.809401Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(f'{ROOT_DIR}/train.csv')\ndf['image_path'] = f'{ROOT_DIR}/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\ndf['annotations'] = df['annotations'].progress_apply(eval)\ndisplay(df.head(2))","metadata":{"papermill":{"duration":0.562594,"end_time":"2022-04-22T02:41:18.052033","exception":false,"start_time":"2022-04-22T02:41:17.489439","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:11:15.811607Z","iopub.execute_input":"2022-04-25T16:11:15.812322Z","iopub.status.idle":"2022-04-25T16:11:16.339498Z","shell.execute_reply.started":"2022-04-25T16:11:15.812290Z","shell.execute_reply":"2022-04-25T16:11:16.338537Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Clean Data\n> Since ~80% (5k) images are without any bbox we will drop them. The model have already been trained so this being done to get some visualizations on the boxes predicted by s6, n6 and ensemble model of the two.","metadata":{"papermill":{"duration":0.036732,"end_time":"2022-04-22T02:41:18.126151","exception":false,"start_time":"2022-04-22T02:41:18.089419","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts(normalize=True)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"papermill":{"duration":0.155373,"end_time":"2022-04-22T02:41:18.31893","exception":false,"start_time":"2022-04-22T02:41:18.163557","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:11:16.340706Z","iopub.execute_input":"2022-04-25T16:11:16.340925Z","iopub.status.idle":"2022-04-25T16:11:16.449891Z","shell.execute_reply.started":"2022-04-25T16:11:16.340898Z","shell.execute_reply":"2022-04-25T16:11:16.448994Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = df.query(\"num_bbox>0\")","metadata":{"papermill":{"duration":0.064393,"end_time":"2022-04-22T02:41:18.499311","exception":false,"start_time":"2022-04-22T02:41:18.434918","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:11:16.451272Z","iopub.execute_input":"2022-04-25T16:11:16.451507Z","iopub.status.idle":"2022-04-25T16:11:16.468545Z","shell.execute_reply.started":"2022-04-25T16:11:16.451477Z","shell.execute_reply":"2022-04-25T16:11:16.467928Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Create BBox\nSince an image can have multiple bounding boxes we are just converting this information in the dataframe to a list type","metadata":{"papermill":{"duration":0.03788,"end_time":"2022-04-22T02:41:19.68037","exception":false,"start_time":"2022-04-22T02:41:19.64249","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes","metadata":{"_kg_hide-input":false,"papermill":{"duration":0.990782,"end_time":"2022-04-22T02:41:19.604837","exception":false,"start_time":"2022-04-22T02:41:18.614055","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:14:18.555823Z","iopub.execute_input":"2022-04-25T16:14:18.556239Z","iopub.status.idle":"2022-04-25T16:14:18.561654Z","shell.execute_reply.started":"2022-04-25T16:14:18.556199Z","shell.execute_reply":"2022-04-25T16:14:18.560458Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df['bboxes'] = df.annotations.progress_apply(get_bbox)\ndf.head(2)","metadata":{"papermill":{"duration":0.116847,"end_time":"2022-04-22T02:41:19.835394","exception":false,"start_time":"2022-04-22T02:41:19.718547","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:14:18.608190Z","iopub.execute_input":"2022-04-25T16:14:18.609101Z","iopub.status.idle":"2022-04-25T16:14:18.806360Z","shell.execute_reply.started":"2022-04-25T16:14:18.608973Z","shell.execute_reply":"2022-04-25T16:14:18.805453Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling","metadata":{"papermill":{"duration":0.039795,"end_time":"2022-04-22T02:41:20.095431","exception":false,"start_time":"2022-04-22T02:41:20.055636","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from bbox.utils import coco2yolo, coco2voc, voc2yolo, voc2coco\nfrom bbox.utils import draw_bboxes, load_image\nfrom bbox.utils import clip_bbox, str2annot, annot2str\n\n\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None,score=None):\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    label=label+\"{:.2f}%\".format(score)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo',\\\n                class_name = False, line_thickness = 2,scores=None): \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            score   = scores[idx]*100\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness,score=score)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            score   = scores[idx]*100\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness,score=score)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\ndef show_img(img, bboxes, bbox_format='yolo',scores=None):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2,scores=scores)\n    return Image.fromarray(img).resize((600, 300))\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"papermill":{"duration":0.093036,"end_time":"2022-04-22T02:41:20.318623","exception":false,"start_time":"2022-04-22T02:41:20.225587","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:14:18.808642Z","iopub.execute_input":"2022-04-25T16:14:18.808977Z","iopub.status.idle":"2022-04-25T16:14:18.857566Z","shell.execute_reply.started":"2022-04-25T16:14:18.808933Z","shell.execute_reply":"2022-04-25T16:14:18.856018Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def load_model(ckpt_path):\n    model = torch.hub.load('/kaggle/input/yolov5-lib-ds/','custom',path=ckpt_path, source='local',force_reload=True)\n    model.conf = 0.25 \n    model.iou  = 0.4\n    model.classes = None   \n    model.multi_label = False \n    model.max_det = 1000  \n    return model\n\ndef get_prediction(model, img):\n    results = model(img, size=3000, augment=True)\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes) > 0:\n        return bboxes, preds['confidence'].values\n    return [], []\n\ndef wbf(bboxes, confs):\n    boxes =  [bbox/1280 for bbox in bboxes]\n    scores = [conf for conf in confs]\n    labels = [np.ones(conf.shape[0]) for conf in confs]\n    \n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=[1,1], iou_thr=0.2, skip_box_thr=0.001)\n    \n    boxes = boxes*(1280-1)\n    return boxes, scores, labels","metadata":{"papermill":{"duration":0.052007,"end_time":"2022-04-22T02:41:20.410161","exception":false,"start_time":"2022-04-22T02:41:20.358154","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:14:18.858863Z","iopub.execute_input":"2022-04-25T16:14:18.860300Z","iopub.status.idle":"2022-04-25T16:14:18.870741Z","shell.execute_reply.started":"2022-04-25T16:14:18.860259Z","shell.execute_reply":"2022-04-25T16:14:18.869920Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"!mkdir /root/.config/Ultralytics\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/Arial.ttf","metadata":{"papermill":{"duration":0.808778,"end_time":"2022-04-22T02:41:21.258425","exception":false,"start_time":"2022-04-22T02:41:20.449647","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:14:18.872566Z","iopub.execute_input":"2022-04-25T16:14:18.872900Z","iopub.status.idle":"2022-04-25T16:14:20.437465Z","shell.execute_reply.started":"2022-04-25T16:14:18.872869Z","shell.execute_reply":"2022-04-25T16:14:20.436202Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"m1 = load_model('/kaggle/input/yolowts/best.pt')\nm2 = load_model('/kaggle/input/yolowts/best-2.pt') ","metadata":{"papermill":{"duration":21.591476,"end_time":"2022-04-22T02:41:42.891349","exception":true,"start_time":"2022-04-22T02:41:21.299873","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:14:20.439282Z","iopub.execute_input":"2022-04-25T16:14:20.439741Z","iopub.status.idle":"2022-04-25T16:14:23.136783Z","shell.execute_reply.started":"2022-04-25T16:14:20.439709Z","shell.execute_reply":"2022-04-25T16:14:23.135944Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"paths = df[df.num_bbox>1].image_path.tolist()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:14:23.138254Z","iopub.execute_input":"2022-04-25T16:14:23.139032Z","iopub.status.idle":"2022-04-25T16:14:23.149184Z","shell.execute_reply.started":"2022-04-25T16:14:23.138973Z","shell.execute_reply":"2022-04-25T16:14:23.148216Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Visualization of predictions made my WBF ensemble model\n\nFor visualization purposes only, we take our training data and compare COTS prediction using 3 models - YOLOV5s6, YOLOV5s6 and WBF ensemble model","metadata":{}},{"cell_type":"code","source":"count = 0\n\nfor p in paths:\n    img = cv2.imread(p)[...,::-1]\n    b1, c1 = get_prediction(m1, img)\n    b2, c2 = get_prediction(m2, img)\n    if len(b1) > 0:\n        b1 = np.array(b1)\n    if len(b2) > 0:\n        b2 = np.array(b2)\n    \n    # voc  => [x1, y1, x2, y2]\n    # coco => [xmin, ymin, w, h]\n    # model predictions are in voc format\n    # for testing we need predictions in coco format\n    if len(b1) > 0 and len(b2) > 0:\n        b, c, labels = wbf([b1, b2], [c1, c2])\n        b = voc2coco(b).astype(int)\n    elif len(b1) > 0:\n        b, c = voc2coco(b1, image_height=720, image_width=1280), c1\n    elif len(b2) > 0:\n        b, c = voc2coco(b2, image_height=720, image_width=1280), c2\n    else:\n        b, c = [], []\n\n    if True:\n        print('\\n\\nYOLOV5s6 Predictions ')\n        if len(b1) > 0:            \n            display(show_img(img, b1, bbox_format='voc_pascal',scores=c1))\n        else:        \n            display(show_img(img, [], bbox_format='voc_pascal',scores=c1))\n\n        print('\\n\\nYoloV5n6 Predictions ')  \n        if len(b2) > 0:\n            display(show_img(img, b2, bbox_format='voc_pascal',scores=c2))\n        else:\n            display(show_img(img, [], bbox_format='voc_pascal',scores=c2))\n\n        print('\\n\\nEnsemble (WBF) Predictions ')\n        if len(b) > 0: \n            display(show_img(img, b, bbox_format='coco',scores=c))\n        else:\n            display(show_img(img, [], bbox_format='coco',scores=None))\n    count += 1\n    if count == 3:\n        break","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:14:23.150390Z","iopub.execute_input":"2022-04-25T16:14:23.150616Z","iopub.status.idle":"2022-04-25T16:14:52.224255Z","shell.execute_reply.started":"2022-04-25T16:14:23.150589Z","shell.execute_reply":"2022-04-25T16:14:52.223528Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Submission\nThe test dataset comprises of about 13000 images and is completely hidden. We have to follow the instructions here https://www.kaggle.com/competitions/tensorflow-great-barrier-reef/overview/evaluation to create a environment and get the iterator of this image set. We then predict the bounding box coordinates in COCO format and the confidences and add this to a dataframe. \n\nTo get the final F2 scores for our model, we submit this notebook to the competition which runs the notebook and evaluates the prediction in the dataframe (written to a submisssion.csv file in /kaggle/working dir)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"%cd /kaggle/working/","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:14:52.225585Z","iopub.execute_input":"2022-04-25T16:14:52.225980Z","iopub.status.idle":"2022-04-25T16:14:52.231301Z","shell.execute_reply.started":"2022-04-25T16:14:52.225950Z","shell.execute_reply":"2022-04-25T16:14:52.230394Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()\niter_test = env.iter_test()  ","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:14:52.232496Z","iopub.execute_input":"2022-04-25T16:14:52.233141Z","iopub.status.idle":"2022-04-25T16:14:52.276774Z","shell.execute_reply.started":"2022-04-25T16:14:52.233103Z","shell.execute_reply":"2022-04-25T16:14:52.275412Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for (img, prediction_df) in iter_test:\n    b1, c1 = get_prediction(m1, img)\n    b2, c2 = get_prediction(m2, img)\n    \n    if len(b1) > 0:\n        b1 = np.array(b1)\n    if len(b2) > 0:\n        b2 = np.array(b2)\n    \n    if len(b1) > 0 and len(b2) > 0:\n        b, c, labels = wbf([b1, b2], [c1, c2])\n        b = voc2coco(b).astype(int)\n    elif len(b1) > 0:\n        b, c = voc2coco(b1, 720, 1280), c1\n    elif len(b2) > 0:\n        b, c = voc2coco(b2, 720, 1280), c2\n    else:\n        b = []\n    \n    predictions = list()\n    for i in range(len(b)):\n        box = b[i]        \n        score = c[i]\n        if score > 0.1:\n            x, y = int(box[0]), int(box[1])\n            width, height = int(box[2]), int(box[3])\n            predictions.append('{:.2f} {} {} {} {}'.format(score, x, y, width, height))\n    print(predictions)\n    prediction_df['annotations'] = ' '.join(predictions)\n    env.predict(prediction_df)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-04-25T16:14:52.279412Z","iopub.execute_input":"2022-04-25T16:14:52.279917Z","iopub.status.idle":"2022-04-25T16:15:33.789413Z","shell.execute_reply.started":"2022-04-25T16:14:52.279884Z","shell.execute_reply":"2022-04-25T16:15:33.788140Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Reference:\nhttps://www.kaggle.com/code/mahipalsingh/gbr-yolox-yolov5-ensemble-2-o","metadata":{}}]}