{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033246,
     "end_time": "2022-04-22T02:40:41.128819",
     "exception": false,
     "start_time": "2022-04-22T02:40:41.095573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-04-22T13:40:23.596279Z",
     "iopub.status.busy": "2022-04-22T13:40:23.595631Z",
     "iopub.status.idle": "2022-04-22T13:40:23.615009Z",
     "shell.execute_reply": "2022-04-22T13:40:23.614082Z",
     "shell.execute_reply.started": "2022-04-22T13:40:23.596182Z"
    },
    "papermill": {
     "duration": 0.041853,
     "end_time": "2022-04-22T02:40:41.20223",
     "exception": false,
     "start_time": "2022-04-22T02:40:41.160377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -qU bbox-utility\n",
    "# !pip download bbox-utility\n",
    "# !pip download ensemble-boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:40:23.673201Z",
     "iopub.status.busy": "2022-04-22T13:40:23.672915Z",
     "iopub.status.idle": "2022-04-22T13:40:51.126276Z",
     "shell.execute_reply": "2022-04-22T13:40:51.12508Z",
     "shell.execute_reply.started": "2022-04-22T13:40:23.67317Z"
    },
    "papermill": {
     "duration": 32.573531,
     "end_time": "2022-04-22T02:41:13.807061",
     "exception": false,
     "start_time": "2022-04-22T02:40:41.23353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Requirement '/kaggle/input/icevision-essentials/icevision-rCHSs/repos/loguru-0.6.0-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\n",
      "Looking in links: /kaggle/input/\n",
      "Processing /kaggle/input/icevision-essentials/icevision-rCHSs/repos/loguru-0.6.0-py3-none-any.whl\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/icevision-essentials/icevision-rCHSs/repos/loguru-0.6.0-py3-none-any.whl'\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: Requirement '/kaggle/input/modules/bbox_utility-1.0.13-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\n",
      "Looking in links: /kaggle/input/modules/\n",
      "Processing /kaggle/input/modules/bbox_utility-1.0.13-py3-none-any.whl\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/modules/bbox_utility-1.0.13-py3-none-any.whl'\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: Requirement '/kaggle/input/modules/ensemble_boxes-1.0.9-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\n",
      "Looking in links: /kaggle/input/modules/\n",
      "Processing /kaggle/input/modules/ensemble_boxes-1.0.9-py3-none-any.whl\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/modules/ensemble_boxes-1.0.9-py3-none-any.whl'\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: Requirement 'commonmark-0.9.1-py2.py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\n",
      "Processing ./commonmark-0.9.1-py2.py3-none-any.whl\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/Users/dylangnatz/Coding_Projects/applied_ML/COTS-Detection/Code/commonmark-0.9.1-py2.py3-none-any.whl'\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: Requirement 'rich-9.13.0-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\n",
      "Processing ./rich-9.13.0-py3-none-any.whl\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/Users/dylangnatz/Coding_Projects/applied_ML/COTS-Detection/Code/rich-9.13.0-py3-none-any.whl'\n",
      "\u001b[0m\n",
      "Requirement already satisfied: norfair[metrics,video] in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (0.1.8)\n",
      "\u001b[33mWARNING: norfair 0.1.8 does not provide the extra 'metrics'\u001b[0m\n",
      "\u001b[33mWARNING: norfair 0.1.8 does not provide the extra 'video'\u001b[0m\n",
      "Requirement already satisfied: opencv-python<5.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from norfair[metrics,video]) (4.5.5.64)\n",
      "Requirement already satisfied: rich<7.0.0,>=6.1.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from norfair[metrics,video]) (6.2.0)\n",
      "Requirement already satisfied: filterpy<2.0.0,>=1.4.5 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from norfair[metrics,video]) (1.4.5)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from filterpy<2.0.0,>=1.4.5->norfair[metrics,video]) (3.3.4)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from filterpy<2.0.0,>=1.4.5->norfair[metrics,video]) (1.19.5)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from filterpy<2.0.0,>=1.4.5->norfair[metrics,video]) (1.5.4)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from rich<7.0.0,>=6.1.1->norfair[metrics,video]) (0.4.4)\n",
      "Requirement already satisfied: dataclasses<0.8,>=0.7 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from rich<7.0.0,>=6.1.1->norfair[metrics,video]) (0.7)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from rich<7.0.0,>=6.1.1->norfair[metrics,video]) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from rich<7.0.0,>=6.1.1->norfair[metrics,video]) (2.11.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0,>=3.7.4 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from rich<7.0.0,>=6.1.1->norfair[metrics,video]) (3.7.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->norfair[metrics,video]) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->norfair[metrics,video]) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/dylangnatz/Library/Python/3.6/lib/python/site-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->norfair[metrics,video]) (2.8.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->norfair[metrics,video]) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->norfair[metrics,video]) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib->filterpy<2.0.0,>=1.4.5->norfair[metrics,video]) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index --find-links '/kaggle/input/' '/kaggle/input/icevision-essentials/icevision-rCHSs/repos/loguru-0.6.0-py3-none-any.whl'\n",
    "!pip install --no-index --find-links '/kaggle/input/modules/' '/kaggle/input/modules/bbox_utility-1.0.13-py3-none-any.whl'\n",
    "!pip install --no-index --find-links '/kaggle/input/modules/' '/kaggle/input/modules/ensemble_boxes-1.0.9-py3-none-any.whl'\n",
    "#Install norfair\n",
    "! pip install norfair[metrics,video]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037393,
     "end_time": "2022-04-22T02:41:13.880401",
     "exception": false,
     "start_time": "2022-04-22T02:41:13.843008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-22T13:40:51.131399Z",
     "iopub.status.busy": "2022-04-22T13:40:51.131112Z",
     "iopub.status.idle": "2022-04-22T13:40:51.481057Z",
     "shell.execute_reply": "2022-04-22T13:40:51.480372Z",
     "shell.execute_reply.started": "2022-04-22T13:40:51.131364Z"
    },
    "papermill": {
     "duration": 0.529984,
     "end_time": "2022-04-22T02:41:14.447647",
     "exception": false,
     "start_time": "2022-04-22T02:41:13.917663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('../input/tensorflow-great-barrier-reef')\n",
    "sys.path.append('/kaggle/input/weightedboxesfusion/')\n",
    "\n",
    "from IPython.display import display\n",
    "from ensemble_boxes import *\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:40:52.785123Z",
     "iopub.status.busy": "2022-04-22T13:40:52.784878Z",
     "iopub.status.idle": "2022-04-22T13:40:52.791068Z",
     "shell.execute_reply": "2022-04-22T13:40:52.789896Z",
     "shell.execute_reply.started": "2022-04-22T13:40:52.785095Z"
    },
    "papermill": {
     "duration": 0.044344,
     "end_time": "2022-04-22T02:41:15.683462",
     "exception": false,
     "start_time": "2022-04-22T02:41:15.639118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:40:54.312086Z",
     "iopub.status.busy": "2022-04-22T13:40:54.311404Z",
     "iopub.status.idle": "2022-04-22T13:40:54.829661Z",
     "shell.execute_reply": "2022-04-22T13:40:54.828604Z",
     "shell.execute_reply.started": "2022-04-22T13:40:54.312037Z"
    },
    "papermill": {
     "duration": 0.562594,
     "end_time": "2022-04-22T02:41:18.052033",
     "exception": false,
     "start_time": "2022-04-22T02:41:17.489439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{ROOT_DIR}/train.csv')\n",
    "df['old_image_path'] = f'{ROOT_DIR}/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\n",
    "df['annotations'] = df['annotations'].progress_apply(eval)\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036732,
     "end_time": "2022-04-22T02:41:18.126151",
     "exception": false,
     "start_time": "2022-04-22T02:41:18.089419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Number of BBoxes\n",
    "> Nearly 80% images are without any bbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:40:54.831428Z",
     "iopub.status.busy": "2022-04-22T13:40:54.831111Z",
     "iopub.status.idle": "2022-04-22T13:40:54.942185Z",
     "shell.execute_reply": "2022-04-22T13:40:54.941059Z",
     "shell.execute_reply.started": "2022-04-22T13:40:54.831383Z"
    },
    "papermill": {
     "duration": 0.155373,
     "end_time": "2022-04-22T02:41:18.31893",
     "exception": false,
     "start_time": "2022-04-22T02:41:18.163557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\n",
    "data = (df.num_bbox>0).value_counts(normalize=True)*100\n",
    "print(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038282,
     "end_time": "2022-04-22T02:41:18.396216",
     "exception": false,
     "start_time": "2022-04-22T02:41:18.357934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clean Data\n",
    "* In this notebook, we use only **bboxed-images** (`~5k`). We can use all `~23K` images for train but most of them don't have any labels. So it would be easier to carry out experiments using only **bboxed images**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:40:54.944058Z",
     "iopub.status.busy": "2022-04-22T13:40:54.943734Z",
     "iopub.status.idle": "2022-04-22T13:40:54.968504Z",
     "shell.execute_reply": "2022-04-22T13:40:54.967785Z",
     "shell.execute_reply.started": "2022-04-22T13:40:54.944016Z"
    },
    "papermill": {
     "duration": 0.064393,
     "end_time": "2022-04-22T02:41:18.499311",
     "exception": false,
     "start_time": "2022-04-22T02:41:18.434918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.query(\"num_bbox>0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038096,
     "end_time": "2022-04-22T02:41:18.575721",
     "exception": false,
     "start_time": "2022-04-22T02:41:18.537625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2022-04-22T13:40:54.970592Z",
     "iopub.status.busy": "2022-04-22T13:40:54.970246Z",
     "iopub.status.idle": "2022-04-22T13:40:55.88085Z",
     "shell.execute_reply": "2022-04-22T13:40:55.879903Z",
     "shell.execute_reply.started": "2022-04-22T13:40:54.97053Z"
    },
    "papermill": {
     "duration": 0.990782,
     "end_time": "2022-04-22T02:41:19.604837",
     "exception": false,
     "start_time": "2022-04-22T02:41:18.614055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bbox.utils import coco2yolo, coco2voc, voc2yolo\n",
    "from bbox.utils import draw_bboxes, load_image\n",
    "from bbox.utils import clip_bbox, str2annot, annot2str\n",
    "\n",
    "def get_bbox(annots):\n",
    "    bboxes = [list(annot.values()) for annot in annots]\n",
    "    return bboxes\n",
    "\n",
    "np.random.seed(32)\n",
    "colors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n",
    "          for idx in range(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03788,
     "end_time": "2022-04-22T02:41:19.68037",
     "exception": false,
     "start_time": "2022-04-22T02:41:19.64249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create BBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:40:55.884631Z",
     "iopub.status.busy": "2022-04-22T13:40:55.884255Z",
     "iopub.status.idle": "2022-04-22T13:40:56.102663Z",
     "shell.execute_reply": "2022-04-22T13:40:56.102016Z",
     "shell.execute_reply.started": "2022-04-22T13:40:55.884583Z"
    },
    "papermill": {
     "duration": 0.116847,
     "end_time": "2022-04-22T02:41:19.835394",
     "exception": false,
     "start_time": "2022-04-22T02:41:19.718547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['bboxes'] = df.annotations.progress_apply(get_bbox)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039795,
     "end_time": "2022-04-22T02:41:20.095431",
     "exception": false,
     "start_time": "2022-04-22T02:41:20.055636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:40:56.136663Z",
     "iopub.status.busy": "2022-04-22T13:40:56.136179Z",
     "iopub.status.idle": "2022-04-22T13:40:56.186859Z",
     "shell.execute_reply": "2022-04-22T13:40:56.185767Z",
     "shell.execute_reply.started": "2022-04-22T13:40:56.136627Z"
    },
    "papermill": {
     "duration": 0.093036,
     "end_time": "2022-04-22T02:41:20.318623",
     "exception": false,
     "start_time": "2022-04-22T02:41:20.225587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def voc2yolo(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    voc  => [x1, y1, x2, y2]\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n",
    "    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n",
    "    \n",
    "    w = bboxes[..., 2] - bboxes[..., 0]\n",
    "    h = bboxes[..., 3] - bboxes[..., 1]\n",
    "    \n",
    "    bboxes[..., 0] = bboxes[..., 0] + w/2\n",
    "    bboxes[..., 1] = bboxes[..., 1] + h/2\n",
    "    bboxes[..., 2] = w\n",
    "    bboxes[..., 3] = h\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def yolo2voc(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    voc  => [x1, y1, x2, y2]\n",
    "    \n",
    "    \"\"\" \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n",
    "    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n",
    "    \n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n",
    "    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def coco2yolo(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    # normolizinig\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n",
    "    \n",
    "    # converstion (xmin, ymin) => (xmid, ymid)\n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def yolo2coco(bboxes, image_height=720, image_width=1280):\n",
    "    \"\"\"\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    \n",
    "    \"\"\" \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    # denormalizing\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n",
    "    \n",
    "    # converstion (xmid, ymid) => (xmin, ymin) \n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def voc2coco(bboxes, image_height=720, image_width=1280):\n",
    "    bboxes  = voc2yolo(bboxes, image_height, image_width)\n",
    "    bboxes  = yolo2coco(bboxes, image_height, image_width)\n",
    "    return bboxes\n",
    "\n",
    "def coco2voc(bboxes, image_height=720, image_width=1280):\n",
    "    bboxes  = coco2yolo(bboxes, image_height, image_width)\n",
    "    bboxes  = yolo2voc(bboxes, image_height, image_width)\n",
    "    return bboxes\n",
    "\n",
    "def load_image(image_path):\n",
    "    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=None,score=None):\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    label=label+\"{:.2f}%\".format(score)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "def draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2,scores=None):  \n",
    "     \n",
    "    image = img.copy()\n",
    "    show_classes = classes if show_classes is None else show_classes\n",
    "    colors = (0, 255 ,0) if colors is None else colors\n",
    "    \n",
    "    if bbox_format == 'yolo':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            score   = scores[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes:\n",
    "            \n",
    "                x1 = round(float(bbox[0])*image.shape[1])\n",
    "                y1 = round(float(bbox[1])*image.shape[0])\n",
    "                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n",
    "                h  = round(float(bbox[3])*image.shape[0]/2)\n",
    "\n",
    "                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(get_label(cls)),\n",
    "                             line_thickness = line_thickness,score=score)\n",
    "            \n",
    "    elif bbox_format == 'coco':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            score   = scores[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes:            \n",
    "                x1 = int(round(bbox[0]))\n",
    "                y1 = int(round(bbox[1]))\n",
    "                w  = int(round(bbox[2]))\n",
    "                h  = int(round(bbox[3]))\n",
    "\n",
    "                voc_bbox = (x1, y1, x1+w, y1+h)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(cls_id),\n",
    "                             line_thickness = line_thickness,score=score)\n",
    "\n",
    "    elif bbox_format == 'voc_pascal':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            score   = scores[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes: \n",
    "                x1 = int(round(bbox[0]))\n",
    "                y1 = int(round(bbox[1]))\n",
    "                x2 = int(round(bbox[2]))\n",
    "                y2 = int(round(bbox[3]))\n",
    "                voc_bbox = (x1, y1, x2, y2)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(cls_id),\n",
    "                             line_thickness = line_thickness,score=score)\n",
    "    else:\n",
    "        raise ValueError('wrong bbox format')\n",
    "\n",
    "    return image\n",
    "\n",
    "def show_img(img, bboxes, bbox_format='yolo',scores=None):\n",
    "    names  = ['starfish']*len(bboxes)\n",
    "    labels = [0]*len(bboxes)\n",
    "    img    = draw_bboxes(img = img,\n",
    "                           bboxes = bboxes, \n",
    "                           classes = names,\n",
    "                           class_ids = labels,\n",
    "                           class_name = True, \n",
    "                           colors = colors, \n",
    "                           bbox_format = bbox_format,\n",
    "                           line_thickness = 2,scores=scores)\n",
    "    return Image.fromarray(img).resize((800, 400))\n",
    "\n",
    "np.random.seed(32)\n",
    "colors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n",
    "          for idx in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:51:23.137208Z",
     "iopub.status.busy": "2022-04-22T13:51:23.136872Z",
     "iopub.status.idle": "2022-04-22T13:51:23.148528Z",
     "shell.execute_reply": "2022-04-22T13:51:23.147373Z",
     "shell.execute_reply.started": "2022-04-22T13:51:23.137175Z"
    },
    "papermill": {
     "duration": 0.052007,
     "end_time": "2022-04-22T02:41:20.410161",
     "exception": false,
     "start_time": "2022-04-22T02:41:20.358154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(ckpt_path):\n",
    "    model = torch.hub.load('/kaggle/input/yolov5-lib-ds/','custom',path=ckpt_path, source='local',force_reload=True)\n",
    "    model.conf = 0.25 #0.1  \n",
    "    model.iou  = 0.4 #0.2 \n",
    "    model.classes = None   \n",
    "    model.multi_label = False \n",
    "    model.max_det = 1000  \n",
    "    return model\n",
    "\n",
    "def get_prediction(model, img):\n",
    "    results = model(img, size=3000, augment=True)\n",
    "    preds   = results.pandas().xyxy[0]\n",
    "    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n",
    "    print(bboxes)\n",
    "    if len(bboxes) > 0:\n",
    "        return bboxes, preds['confidence'].values\n",
    "    return [], []\n",
    "\n",
    "def wbf(bboxes, confs):\n",
    "    boxes =  [bbox/1280 for bbox in bboxes]\n",
    "    scores = [conf for conf in confs]\n",
    "    labels = [np.ones(conf.shape[0]) for conf in confs]\n",
    "    \n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=[1,1], iou_thr=0.2, skip_box_thr=0.001)\n",
    "    \n",
    "    boxes = boxes*(1280-1)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:40:56.975305Z",
     "iopub.status.busy": "2022-04-22T13:40:56.974332Z",
     "iopub.status.idle": "2022-04-22T13:40:57.763151Z",
     "shell.execute_reply": "2022-04-22T13:40:57.7617Z",
     "shell.execute_reply.started": "2022-04-22T13:40:56.975245Z"
    },
    "papermill": {
     "duration": 0.808778,
     "end_time": "2022-04-22T02:41:21.258425",
     "exception": false,
     "start_time": "2022-04-22T02:41:20.449647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir /root/.config/Ultralytics\n",
    "!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/Arial.ttf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:40:57.765179Z",
     "iopub.status.busy": "2022-04-22T13:40:57.764929Z",
     "iopub.status.idle": "2022-04-22T13:41:00.388696Z",
     "shell.execute_reply": "2022-04-22T13:41:00.387806Z",
     "shell.execute_reply.started": "2022-04-22T13:40:57.765151Z"
    },
    "papermill": {
     "duration": 21.591476,
     "end_time": "2022-04-22T02:41:42.891349",
     "exception": true,
     "start_time": "2022-04-22T02:41:21.299873",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m1 = load_model('/kaggle/input/yolowts/best.pt')\n",
    "m2 = load_model('/kaggle/input/yolowts/best-2.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:41:00.391183Z",
     "iopub.status.busy": "2022-04-22T13:41:00.390821Z",
     "iopub.status.idle": "2022-04-22T13:41:00.402071Z",
     "shell.execute_reply": "2022-04-22T13:41:00.401472Z",
     "shell.execute_reply.started": "2022-04-22T13:41:00.391139Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = df[df.num_bbox>1].old_image_path.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of WBF Ensembling Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:53:35.10761Z",
     "iopub.status.busy": "2022-04-22T13:53:35.107243Z",
     "iopub.status.idle": "2022-04-22T13:53:35.112651Z",
     "shell.execute_reply": "2022-04-22T13:53:35.111798Z",
     "shell.execute_reply.started": "2022-04-22T13:53:35.10755Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for p in paths:\n",
    "#     img = cv2.imread(p)[...,::-1]\n",
    "#     b1, c1 = get_prediction(m1, img)\n",
    "#     b2, c2 = get_prediction(m2, img)\n",
    "#     if len(b1) > 0:\n",
    "#         b1 = np.array(b1)\n",
    "#     if len(b2) > 0:\n",
    "#         b2 = np.array(b2)\n",
    "    \n",
    "#     # voc  => [x1, y1, x2, y2]\n",
    "#     # coco => [xmin, ymin, w, h]\n",
    "#     # model predictions are in voc format\n",
    "#     # for testing we need predictions in coco format\n",
    "#     if len(b1) > 0 and len(b2) > 0:\n",
    "#         b, c, labels = wbf([b1, b2], [c1, c2])\n",
    "#         b = voc2coco(b).astype(int)\n",
    "#     elif len(b1) > 0:\n",
    "#         b, c = voc2coco(b1, image_height=720, image_width=1280), c1\n",
    "#     elif len(b2) > 0:\n",
    "#         b, c = voc2coco(b2, image_height=720, image_width=1280), c2\n",
    "#     else:\n",
    "#         b, c = [], []\n",
    "#     print(b1, c1, b2, c2, b, c)\n",
    "#     print(b, c)\n",
    "#     if True: #len(b1) > 0 and len(b2) > 0:\n",
    "#         print('\\n\\nYOLOV5s6 Predictions ')\n",
    "#         if len(b1) > 0:            \n",
    "#             display(show_img(img, b1, bbox_format='voc_pascal',scores=c1))\n",
    "#         else:        \n",
    "#             display(show_img(img, [], bbox_format='voc_pascal',scores=c1))\n",
    "\n",
    "#         print('\\n\\nYoloV5n6 Predictions ')  \n",
    "#         if len(b2) > 0:\n",
    "#             display(show_img(img, b2, bbox_format='voc_pascal',scores=c2))\n",
    "#         else:\n",
    "#             display(show_img(img, [], bbox_format='voc_pascal',scores=c2))\n",
    "\n",
    "#         print('\\n\\nEnsemble (WBF) Predictions ')\n",
    "#         if len(b) > 0: \n",
    "#             display(show_img(img, b, bbox_format='coco',scores=c))\n",
    "#         else:\n",
    "#             display(show_img(img, [], bbox_format='coco',scores=None))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#                      Tracking helpers                      #\n",
    "##############################################################\n",
    "\n",
    "import numpy as np\n",
    "from norfair import Detection, Tracker\n",
    "\n",
    "# Helper to convert bbox in format [x_min, y_min, x_max, y_max, score] to norfair.Detection class\n",
    "def to_norfair(detects, frame_id):\n",
    "    result = []\n",
    "    for x_min, y_min, x_max, y_max, score in detects:\n",
    "        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n",
    "        w, h = x_max - x_min, y_max - y_min\n",
    "        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\n",
    "def euclidean_distance(detection, tracked_object):\n",
    "    return np.linalg.norm(detection.points - tracked_object.estimate)\n",
    "        \n",
    "submission_dict = {\n",
    "    'id': [],\n",
    "    'prediction_string': [],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:51:51.572485Z",
     "iopub.status.busy": "2022-04-22T13:51:51.572187Z",
     "iopub.status.idle": "2022-04-22T13:51:51.578824Z",
     "shell.execute_reply": "2022-04-22T13:51:51.577873Z",
     "shell.execute_reply.started": "2022-04-22T13:51:51.572451Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:51:53.73784Z",
     "iopub.status.busy": "2022-04-22T13:51:53.73751Z",
     "iopub.status.idle": "2022-04-22T13:51:53.810338Z",
     "shell.execute_reply": "2022-04-22T13:51:53.809538Z",
     "shell.execute_reply.started": "2022-04-22T13:51:53.737805Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import greatbarrierreef\n",
    "env = greatbarrierreef.make_env()\n",
    "iter_test = env.iter_test()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#                      Tracking                       #\n",
    "#######################################################\n",
    "\n",
    "# Tracker will update tracks based on detections from current frame\n",
    "# Matching based on euclidean distance between bbox centers of detections \n",
    "# from current frame and tracked_objects based on previous frames\n",
    "# You can check it's parameters in norfair docs\n",
    "# https://github.com/tryolabs/norfair/blob/master/docs/README.md\n",
    "tracker = Tracker(\n",
    "    distance_function=euclidean_distance, \n",
    "    distance_threshold=30,\n",
    "    hit_inertia_min=3,\n",
    "    hit_inertia_max=6,\n",
    "    initialization_delay=1,\n",
    ")\n",
    "\n",
    "# Save frame_id into detection to know which tracks have no detections on current frame\n",
    "frame_id = 0\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:52:01.823047Z",
     "iopub.status.busy": "2022-04-22T13:52:01.822027Z",
     "iopub.status.idle": "2022-04-22T13:52:28.898534Z",
     "shell.execute_reply": "2022-04-22T13:52:28.897594Z",
     "shell.execute_reply.started": "2022-04-22T13:52:01.823001Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for (image_np, sample_prediction_df) in iter_test:\n",
    " \n",
    "    bboxes, bbclasses, scores = yolox_inference(image_np[:,:,::-1], model, test_size)\n",
    "    \n",
    "    predictions = []\n",
    "    detects = []\n",
    "    for i in range(len(bboxes)):\n",
    "        box = bboxes[i]\n",
    "        cls_id = int(bbclasses[i])\n",
    "        score = scores[i]\n",
    "        if score < confthre:\n",
    "            continue\n",
    "        x_min = int(box[0])\n",
    "        y_min = int(box[1])\n",
    "        x_max = int(box[2])\n",
    "        y_max = int(box[3])\n",
    "        detects.append([x_min, y_min, x_max, y_max, score])\n",
    "        \n",
    "        bbox_width = x_max - x_min\n",
    "        bbox_height = y_max - y_min\n",
    "        \n",
    "        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n",
    "    \n",
    "    #######################################################\n",
    "    #                      Tracking                       #\n",
    "    #######################################################\n",
    "    \n",
    "    # Update tracks using detects from current frame\n",
    "    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n",
    "    for tobj in tracked_objects:\n",
    "        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n",
    "        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n",
    "            continue\n",
    "            \n",
    "        # Add objects that have no detections on current frame to predictions\n",
    "        xc, yc = tobj.estimate[0]\n",
    "        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n",
    "        score = tobj.last_detection.scores[0]\n",
    "\n",
    "        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n",
    "    #######################################################\n",
    "    \n",
    "    prediction_str = ' '.join(predictions)\n",
    "    sample_prediction_df['annotations'] = prediction_str\n",
    "    env.predict(sample_prediction_df)\n",
    "\n",
    "    print('Prediction:', prediction_str)\n",
    "    frame_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('submission.csv')\n",
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
